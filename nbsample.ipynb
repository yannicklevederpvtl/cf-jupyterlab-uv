{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af95a1e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238c187-07d2-479d-96dc-bd67c82fee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "\n",
    "httpx_client = httpx.Client(http2=True, verify=False)\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "\n",
    "openai = OpenAI(base_url=chat_credentials[\"api_base\"], api_key=chat_credentials[\"api_key\"], http_client=httpx_client)\n",
    "\n",
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a joke for an audience of Cloud Foundry enthusiats\"\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]\n",
    "\n",
    "completion = openai.chat.completions.create(model=chat_credentials[\"model_name\"], messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4ad05",
   "metadata": {},
   "source": [
    "Add 'matplotlib' dependency using the Terminal and 'uv pip install matplotlib' command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22935fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Ask AI for data in a specific format\n",
    "response = openai.chat.completions.create(\n",
    "    model=chat_credentials[\"model_name\"],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"\"\"Give me 5 fictional Cloud Foundry application names with their memory usage in MB. \n",
    "    Format your response exactly like this JSON:\n",
    "    {\"applications\": [{\"name\": \"AppName\", \"memory_mb\": 512, \"instances\": 3, \"space\": \"development\"}]}\n",
    "    \n",
    "    Make the app names realistic like microservices (user-service, payment-api, etc.) and memory between 256-2048 MB.\"\"\"}]\n",
    ")\n",
    "\n",
    "print(\"AI Response:\", response.choices[0].message.content)\n",
    "\n",
    "try:\n",
    "    # Try to parse as JSON\n",
    "    ai_text = response.choices[0].message.content\n",
    "    \n",
    "    # Extract JSON from response (in case there's extra text)\n",
    "    start = ai_text.find('{')\n",
    "    end = ai_text.rfind('}') + 1\n",
    "    json_str = ai_text[start:end]\n",
    "    \n",
    "    data = json.loads(json_str)\n",
    "    app_names = [app[\"name\"] for app in data[\"applications\"]]\n",
    "    memory_usage = [app[\"memory_mb\"] for app in data[\"applications\"]]\n",
    "    instances = [app.get(\"instances\", random.randint(1, 5)) for app in data[\"applications\"]]\n",
    "    spaces = [app.get(\"space\", \"production\") for app in data[\"applications\"]]\n",
    "    \n",
    "except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
    "    print(f\"JSON parsing failed: {e}\")\n",
    "    print(\"Using fallback Cloud Foundry data\")\n",
    "    app_names = ['user-service', 'payment-api', 'notification-worker', 'auth-gateway', 'analytics-engine']\n",
    "    memory_usage = [random.randint(256, 2048) for _ in app_names]\n",
    "    instances = [random.randint(1, 5) for _ in app_names]\n",
    "    spaces = ['production', 'staging', 'development', 'production', 'staging']\n",
    "\n",
    "print(f\"Applications: {app_names}\")\n",
    "print(f\"Memory Usage (MB): {memory_usage}\")\n",
    "print(f\"Instances: {instances}\")\n",
    "print(f\"Spaces: {spaces}\")\n",
    "\n",
    "# Create the main chart - Memory Usage\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Color mapping for Cloud Foundry spaces\n",
    "space_colors = {\n",
    "    'production': '#e74c3c',    # Red\n",
    "    'staging': '#f39c12',       # Orange  \n",
    "    'development': '#27ae60',   # Green\n",
    "    'testing': '#3498db'        # Blue\n",
    "}\n",
    "\n",
    "colors = [space_colors.get(space, '#95a5a6') for space in spaces]\n",
    "\n",
    "# Chart 1: Memory Usage by Application\n",
    "bars1 = ax1.bar(app_names, memory_usage, color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax1.set_title('Cloud Foundry Applications - Memory Allocation', fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_ylabel('Memory Usage (MB)', fontsize=12)\n",
    "ax1.set_xlabel('Applications', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars with instance count\n",
    "for bar, memory, instance_count, space in zip(bars1, memory_usage, instances, spaces):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + max(memory_usage)*0.01, \n",
    "             f'{memory} MB\\n({instance_count} instances)', \n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Add space label at the bottom of each bar\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height * 0.1, \n",
    "             space.upper(), \n",
    "             ha='center', va='bottom', fontsize=8, color='white', fontweight='bold')\n",
    "\n",
    "# Add grid for better readability\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "# Chart 2: Instance Count by Application\n",
    "bars2 = ax2.bar(app_names, instances, color=colors, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax2.set_title('Cloud Foundry Applications - Instance Count', fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.set_ylabel('Number of Instances', fontsize=12)\n",
    "ax2.set_xlabel('Applications', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, instance_count in zip(bars2, instances):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height + 0.05, \n",
    "             str(instance_count), \n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add grid for better readability\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.set_axisbelow(True)\n",
    "ax2.set_ylim(0, max(instances) + 1)\n",
    "\n",
    "# Create custom legend for spaces\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=space.title()) \n",
    "                  for space, color in space_colors.items() if space in spaces]\n",
    "\n",
    "ax1.legend(handles=legend_elements, title='CF Spaces', loc='upper right', \n",
    "          bbox_to_anchor=(1, 1), framealpha=0.9)\n",
    "\n",
    "# Add summary statistics text box\n",
    "total_memory = sum(memory_usage)\n",
    "total_instances = sum(instances)\n",
    "avg_memory = total_memory / len(app_names)\n",
    "\n",
    "stats_text = f\"\"\"Cloud Foundry Summary:\n",
    "â€¢ Total Memory: {total_memory:,} MB\n",
    "â€¢ Total Instances: {total_instances}\n",
    "â€¢ Average Memory/App: {avg_memory:.0f} MB\n",
    "â€¢ Applications: {len(app_names)}\"\"\"\n",
    "\n",
    "# Add text box with statistics\n",
    "ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes, fontsize=10,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional Cloud Foundry specific visualization\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLOUD FOUNDRY DEPLOYMENT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (app, memory, inst, space) in enumerate(zip(app_names, memory_usage, instances, spaces), 1):\n",
    "    status = \"ðŸŸ¢ RUNNING\" if memory < 1000 else \"ðŸŸ¡ HIGH MEMORY\"\n",
    "    print(f\"{i}. {app.upper()}\")\n",
    "    print(f\"   â””â”€ Space: {space}\")\n",
    "    print(f\"   â””â”€ Memory: {memory} MB x {inst} instances = {memory * inst} MB total\")\n",
    "    print(f\"   â””â”€ Status: {status}\")\n",
    "    print()\n",
    "\n",
    "# Create a Cloud Foundry org/space hierarchy visualization\n",
    "print(\"CF ORG/SPACE HIERARCHY:\")\n",
    "print(\"â””â”€ my-org\")\n",
    "for space in set(spaces):\n",
    "    apps_in_space = [app for app, s in zip(app_names, spaces) if s == space]\n",
    "    print(f\"   â”œâ”€ {space}\")\n",
    "    for app in apps_in_space:\n",
    "        print(f\"   â”‚  â””â”€ {app}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f096a85",
   "metadata": {},
   "source": [
    "Add 'gradio' dependency using the Terminal and 'uv pip install gradio' command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_with_ai(message, history):\n",
    "    \"\"\"\n",
    "    Function to handle chat interactions with the AI model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the conversation history for the API\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are an assistant that is great at telling jokes. Always respond directly with the joke or answer. Do not show your thinking process or use thinking tags. Just give the final response immediately.\"}]\n",
    "        \n",
    "        # Add conversation history (now in messages format)\n",
    "        for msg in history:\n",
    "            messages.append(msg)\n",
    "        \n",
    "        # Add the current message\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Make API call with streaming enabled and higher token limit\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=chat_credentials[\"model_name\"], \n",
    "            messages=messages,\n",
    "            max_tokens=800,  # Increased token limit to avoid cutoffs\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Collect the complete streamed response with timeout handling\n",
    "        full_response = \"\"\n",
    "        print(\"=== STREAMING RESPONSE ===\")\n",
    "        \n",
    "        try:\n",
    "            for chunk in completion:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    full_response += content\n",
    "                    print(content, end='', flush=True)\n",
    "                \n",
    "                # Check if we have a complete response (ends with </think> followed by content)\n",
    "                if '</think>' in full_response:\n",
    "                    # Continue streaming to get content after </think>\n",
    "                    continue\n",
    "                elif full_response.strip().endswith('</think>'):\n",
    "                    # Stream ended right at </think>, this might be incomplete\n",
    "                    print(\"\\n[WARNING: Stream ended at </think> - might be incomplete]\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[STREAMING ERROR: {e}]\")\n",
    "        \n",
    "        print(\"\\n=== STREAMING COMPLETE ===\")\n",
    "        response = full_response\n",
    "        print(f\"=== FULL API RESPONSE START ===\")\n",
    "        print(response)\n",
    "        print(f\"=== FULL API RESPONSE END ===\")\n",
    "        print(f\"Response length: {len(response) if response else 0}\")\n",
    "        print(f\"Response type: {type(response)}\")\n",
    "        \n",
    "        # Clean the response - simply extract what comes after </think>\n",
    "        cleaned_response = \"\"\n",
    "        \n",
    "        if response and '</think>' in response:\n",
    "            # Find the last </think> tag and get everything after it\n",
    "            think_end_index = response.rfind('</think>')\n",
    "            after_think = response[think_end_index + len('</think>'):].strip()\n",
    "            cleaned_response = after_think\n",
    "            print(f\"Content after </think>: '{cleaned_response}'\")\n",
    " \n",
    "        print(f\"Cleaned Response: {cleaned_response}\")  # Debug print\n",
    "        return cleaned_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        print(f\"Error occurred: {error_msg}\")  # Debug print\n",
    "        return error_msg\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"AI Joke Assistant\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– AI Joke Assistant\")\n",
    "    gr.Markdown(\"Chat with an AI that's great at telling jokes! Ask for jokes about any topic.\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        height=400,\n",
    "        placeholder=\"Start chatting with your AI joke assistant...\",\n",
    "        show_copy_button=True,\n",
    "        type=\"messages\"  # Use the new messages format\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(\n",
    "            placeholder=\"Ask for a joke or chat with the AI...\",\n",
    "            show_label=False,\n",
    "            scale=4\n",
    "        )\n",
    "        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "    \n",
    "    with gr.Row():\n",
    "        clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # Handle sending messages\n",
    "    def respond(message, chat_history):\n",
    "        print(f\"User message: {message}\")  # Debug print\n",
    "        print(f\"Current history length: {len(chat_history)}\")  # Debug print\n",
    "        \n",
    "        if not message.strip():\n",
    "            return \"\", chat_history\n",
    "        \n",
    "        bot_message = chat_with_ai(message, chat_history)\n",
    "        print(f\"Bot response: {bot_message}\")  # Debug print\n",
    "        \n",
    "        # Add user message and bot response to history (using messages format)\n",
    "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_message})\n",
    "        \n",
    "        print(f\"Updated history length: {len(chat_history)}\")  # Debug print\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    send_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear_btn.click(lambda: [], None, chatbot, queue=False)\n",
    "    \n",
    "    # Example prompts\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"Tell a joke for Cloud Foundry enthusiasts\",\n",
    "            \"Give me a programming joke\",\n",
    "            \"Tell me a dad joke\",\n",
    "            \"What's a funny joke about containers?\",\n",
    "            \"Make me laugh with a tech joke\"\n",
    "        ],\n",
    "        inputs=msg\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(\n",
    "    share=True,  # Creates a public link\n",
    "    server_name=\"0.0.0.0\",  # Allows access from any IP\n",
    "    server_port=7860,  # Default Gradio port\n",
    "    show_error=True,\n",
    "    quiet=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
